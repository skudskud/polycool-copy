# ðŸ”„ Architecture des Services de Data Ingestion

**Date:** Novembre 2025
**Version:** 1.0
**Auteur:** Senior Software Engineer

---

## ðŸ“‹ Vue d'ensemble

Ce document analyse en dÃ©tail les **3 services principaux de data ingestion** : Poller, Streamer et Indexer Subsquid. Pour chaque service, nous examinerons :

- ðŸŽ¯ **Setup & Configuration**
- ðŸ”— **Relations** avec bot, Redis, Supabase
- ðŸ’¡ **UtilitÃ©** et cas d'usage
- âŒ **Critiques** et points faibles
- ðŸ”§ **AmÃ©liorations** proposÃ©es

---

## ðŸ”„ 1. POLLER SERVICE - Polling Gamma API

### ðŸŽ¯ **Setup & Configuration**

#### **Architecture Technique**
```python
# data-ingestion/src/config.py
class Settings:
    POLLER_ENABLED = os.getenv('POLLER_ENABLED', 'true').lower() == 'true'
    POLL_MS = int(os.getenv('POLL_MS', '60000'))  # 60 secondes
    POLL_RATE_LIMIT_BACKOFF_MAX = 300  # 5 minutes max backoff
    GAMMA_API_URL = "https://gamma-api.polymarket.com"
    EXPERIMENTAL_SUBSQUID = os.getenv('EXPERIMENTAL_SUBSQUID', 'true').lower() == 'true'
```

#### **DÃ©marrage & Cycle**
```python
# data-ingestion/src/polling/poller.py
async def start(self):
    """Boucle principale de polling"""
    while True:
        await self.poll_cycle()
        await asyncio.sleep(settings.POLL_MS / 1000.0)
```

#### **Approche Hybride (2 Passes)**
```python
# PASS 1: /events endpoint (marchÃ©s groupÃ©s)
events = await self._fetch_events(offset, limit)  # DonnÃ©es complÃ¨tes
for event in events:
    markets = event.get("markets", [])
    enriched = self._enrich_market_from_event(market, event)

# PASS 2: /markets endpoint (marchÃ©s manquÃ©s)
standalone = await self._fetch_markets()  # Individuel si pas couvert
```

### ðŸ”— **Relations avec le SystÃ¨me**

#### **Avec le Bot**
```
Poller â†’ MarketDataLayer â†’ Bot Commands (/markets, /positions)
    â†“
subsquid_markets_poll â†’ Redis Cache â†’ UI Response
```

#### **Avec Redis**
```python
# Cache les donnÃ©es enrichies
redis_cache.cache_markets_page(filter_name, page, markets, ttl=600)

# TTL: 10 minutes pour marchÃ©s (changement lent)
```

#### **Avec Supabase**
```sql
-- Table principale
CREATE TABLE subsquid_markets_poll (
    market_id TEXT PRIMARY KEY,
    outcomes TEXT[],
    outcome_prices NUMERIC(8,4)[],
    events JSONB,  -- MÃ©tadonnÃ©es d'Ã©vÃ©nements
    updated_at TIMESTAMPTZ DEFAULT now()
);
```

### ðŸ’¡ **UtilitÃ© & Cas d'Usage**

#### **RÃ´le Principal**
- **Source de vÃ©ritÃ©** pour mÃ©tadonnÃ©es marchÃ©s (questions, outcomes, volumes)
- **DonnÃ©es enrichies** avec events, catÃ©gories, tokens
- **Fallback fiable** quand WebSocket indisponible

#### **Cas d'Usage**
- âœ… **/markets command** - Liste complÃ¨te des marchÃ©s
- âœ… **Market details** - Informations complÃ¨tes sur un marchÃ©
- âœ… **Analytics** - Statistiques de volume/liquiditÃ©
- âœ… **Search** - Indexation pour recherche par titre/catÃ©gorie

### âŒ **Critiques & Points Faibles**

#### **Performance**
- âŒ **60s minimum latency** - Pas temps rÃ©el
- âŒ **ETag caching limitÃ©** - Re-tÃ©lÃ©charge souvent
- âŒ **2 passes sÃ©parÃ©es** = complexitÃ© + overhead

#### **FiabilitÃ©**
- âŒ **Race conditions** entre PASS 1 et PASS 2
- âŒ **Categorizer AI cassÃ©** (dÃ©sactivÃ© = perte fonctionnalitÃ©)
- âŒ **Backoff agressif** bloque ingestion pendant erreurs

#### **Maintenance**
- âŒ **Code volumineux** (1800+ lignes)
- âŒ **Configuration fragmentÃ©e** (env vars partout)
- âŒ **Monitoring limitÃ©** (pas de mÃ©triques dÃ©taillÃ©es)

### ðŸ”§ **AmÃ©liorations ProposÃ©es**

#### **PrioritÃ© Haute**
1. **Single Pass Optimization**
   ```python
   # Nouveau: Query optimisÃ©e combinant events + marchÃ©s
   async def poll_unified(self):
       # Une seule requÃªte pour tout couvrir
       # Ã‰limine race conditions
   ```

2. **Intelligent Caching**
   ```python
   # Cache diffÃ©rentiel basÃ© sur changement
   if market_changed_since_last_poll:
       await update_cache(market_id)
   ```

3. **Monitoring Complet**
   ```python
   # MÃ©triques Prometheus
   POLLER_SUCCESS = Counter('poller_success', ['endpoint', 'status'])
   POLLER_LATENCY = Histogram('poller_latency', ['operation'])
   ```

#### **PrioritÃ© Moyenne**
4. **Configuration CentralisÃ©e**
   ```python
   # Config as code au lieu d'env vars
   class PollerConfig:
       endpoints = ['/events', '/markets']
       ttl_by_data_type = {'markets': 600, 'events': 300}
   ```

5. **Error Recovery Intelligent**
   ```python
   # Retry avec circuit breaker par endpoint
   @circuit_breaker(endpoint='/events')
   async def fetch_events_safe(self):
       pass
   ```

---

## ðŸŒ 2. STREAMER SERVICE - WebSocket Temps RÃ©el

### ðŸŽ¯ **Setup & Configuration**

#### **Connexion WebSocket**
```python
# data-ingestion/src/config.py
CLOB_WSS_URL = "wss://ws-subscriptions-clob.polymarket.com"
WS_RECONNECT_BACKOFF_MAX = 300  # 5 minutes
STREAMER_ENABLED = os.getenv('STREAMER_ENABLED', 'true').lower() == 'true'
```

#### **Souscriptions Dynamiques**
```python
# Refresh automatique des souscriptions
self.subscription_refresh_interval = 60  # Toutes les 60s
self.subscription_refresh_task = asyncio.create_task(
    self._periodic_subscription_refresh()
)
```

#### **Gestion des Messages**
```python
# Types de messages supportÃ©s
MESSAGE_TYPES = {
    'snapshot': self._handle_orderbook_snapshot,
    'delta': self._handle_orderbook_delta,
    'trade': self._handle_trade_update
}
```

### ðŸ”— **Relations avec le SystÃ¨me**

#### **Avec le Bot**
```
Streamer â†’ subsquid_markets_ws â†’ MarketDataLayer.get_live_price()
    â†“
/positions refresh â†’ Prix temps rÃ©el (best_bid/ask/mid)
    â†“
UI updates instantanÃ©es
```

#### **Avec Redis**
```python
# Cache ultra-court pour fraÃ®cheur maximale
redis_cache.cache_token_price(token_id, price, ttl=20)  # 20 secondes

# Pas de cache long - donnÃ©es trop volatiles
```

#### **Avec Supabase**
```sql
-- Table temps rÃ©el
CREATE TABLE subsquid_markets_ws (
    market_id TEXT PRIMARY KEY,
    last_bb NUMERIC(8,4),     -- Best bid
    last_ba NUMERIC(8,4),     -- Best ask
    last_mid NUMERIC(8,4),    -- Mid price calculÃ©
    last_trade_price NUMERIC(8,4),
    updated_at TIMESTAMPTZ DEFAULT now()
);
```

### ðŸ’¡ **UtilitÃ© & Cas d'Usage**

#### **RÃ´le Principal**
- **Prix temps rÃ©el** pour trading actif
- **Orderbook live** pour dÃ©cisions trading
- **Trades instantanÃ©s** pour monitoring

#### **Cas d'Usage**
- âœ… **Trading actif** - Prix up-to-date pour dÃ©cisions
- âœ… **Positions monitoring** - Valeur portefeuille temps rÃ©el
- âœ… **Arbitrage** - DÃ©tection Ã©carts prix
- âœ… **Alertes prix** - Notifications quand seuils atteints

### âŒ **Critiques & Points Faibles**

#### **FiabilitÃ©**
- âŒ **Connexion fragile** - DÃ©pend du rÃ©seau
- âŒ **Auto-reconnexion complexe** (backoff + jitter)
- âŒ **Pas de persistence** - DonnÃ©es perdues au restart

#### **Performance**
- âŒ **Souscriptions dynamiques** = overhead rÃ©seau continu
- âŒ **Parsing CPU intensif** - Chaque message traitÃ© individuellement
- âŒ **Memory leaks** potentiels avec reconnexions frÃ©quentes

#### **ObservabilitÃ©**
- âŒ **Logs verbeux** mais pas structurÃ©s
- âŒ **Pas de mÃ©triques** de performance (latence, throughput)
- âŒ **Debugging difficile** (WebSocket state invisible)

### ðŸ”§ **AmÃ©liorations ProposÃ©es**

#### **PrioritÃ© Haute**
1. **Connection Pooling**
   ```python
   # Pool de connexions pour haute disponibilitÃ©
   class WebSocketPool:
       def __init__(self, urls: List[str]):
           self.connections = [WebSocketConnection(url) for url in urls]
   ```

2. **State Persistence**
   ```python
   # Sauvegarde Ã©tat pour recovery rapide
   async def save_streamer_state(self):
       state = {
           'subscriptions': list(self.subscribed_markets),
           'last_message_ts': self.last_message_time
       }
       await redis_client.set('streamer_state', json.dumps(state))
   ```

3. **Batch Processing**
   ```python
   # Accumule updates avant DB write
   async def batch_upsert_prices(self, price_updates: Dict[str, float]):
       # Single bulk upsert au lieu de N individuels
       await db.bulk_update_prices(price_updates)
   ```

#### **PrioritÃ© Moyenne**
4. **Health Checks AvancÃ©s**
   ```python
   # Monitoring connexion + latence
   async def health_check(self):
       latency = await self.measure_latency()
       return {
           'connected': self.websocket is not None,
           'latency_ms': latency,
           'messages_per_sec': self.message_rate
       }
   ```

5. **Configuration Adaptive**
   ```python
   # Ajuste stratÃ©gie selon volatilitÃ© marchÃ©
   def adapt_refresh_rate(self, market_volatility: float):
       if market_volatility > 0.8:  # MarchÃ© volatile
           self.subscription_refresh_interval = 30  # Plus frÃ©quent
       else:
           self.subscription_refresh_interval = 120  # Moins frÃ©quent
   ```

---

## â›“ï¸ 3. INDEXER SUBSQUID - On-Chain Data

### ðŸŽ¯ **Setup & Configuration**

#### **Configuration TypeScript**
```typescript
// indexer-ts/src/main.ts
const TRANSFER_SINGLE_TOPIC = '0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62'
const TRANSFER_BATCH_TOPIC = '0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07ce33e6397d8d63df03e93'

// RPC Configuration
const rpcUrl = process.env.RPC_POLYGON_HTTP || 'https://polygon-rpc.com'
```

#### **Database Connection**
```typescript
// IPv4-first DNS pour Railway
setDefaultResultOrder('ipv4first')
process.env.NODE_OPTIONS += ' --dns-result-order=ipv4first'
```

#### **Webhook Setup**
```typescript
// indexer-ts/src/webhook-notifier.ts
const WEBHOOK_URL = process.env.WEBHOOK_URL
const WEBHOOK_SECRET = process.env.WEBHOOK_SECRET

// Copy trading addresses cache
const watched_cache = new WatchedAddressCache()
```

### ðŸ”— **Relations avec le SystÃ¨me**

#### **Avec le Bot**
```
Indexer â†’ Webhook POST â†’ Bot (copy trading)
    â†“
subsquid_user_transactions â†’ Bot DB â†’ Trading decisions
    â†“
Notifications temps rÃ©el aux utilisateurs
```

#### **Avec Redis**
```python
# Cache adresses surveillÃ©es
await redis_client.setex('watched_addresses:cache:v1', 300, json.dumps(data))

# Cache utilisÃ© par indexer pour filtrage rapide
watched_addresses = await redis_client.get('watched_addresses:cache:v1')
```

#### **Avec Supabase**
```sql
-- Transactions on-chain
CREATE TABLE subsquid_user_transactions (
    id TEXT PRIMARY KEY,
    tx_id TEXT UNIQUE,
    user_address TEXT NOT NULL,
    market_id TEXT,
    outcome INTEGER,  -- 0=NO, 1=YES
    tx_type TEXT,     -- BUY or SELL
    amount NUMERIC(18,8),
    price NUMERIC(8,4),
    amount_in_usdc NUMERIC(18,6),  -- Confusion avec amount!
    tx_hash TEXT,
    block_number BIGINT,
    timestamp TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- Fills blockchain
CREATE TABLE subsquid_fills_onchain (
    fill_id TEXT PRIMARY KEY,
    market_id TEXT,
    user_address TEXT,
    outcome TEXT,
    amount NUMERIC(18,8),
    price NUMERIC(8,4),
    tx_hash TEXT,
    block_number BIGINT,
    timestamp TIMESTAMPTZ
);
```

### ðŸ’¡ **UtilitÃ© & Cas d'Usage**

#### **RÃ´le Principal**
- **Source de vÃ©ritÃ© on-chain** pour transactions utilisateurs
- **Copy trading data** - Trades externes non trackÃ©s par le bot
- **Audit trail complet** des activitÃ©s blockchain

#### **Cas d'Usage**
- âœ… **Copy trading externe** - Suivre traders hors plateforme
- âœ… **Smart wallet tracking** - Monitorer wallets identifiÃ©es
- âœ… **Fraud detection** - VÃ©rifier cohÃ©rence on-chain/off-chain
- âœ… **Analytics avancÃ©s** - P&L rÃ©el basÃ© blockchain

### âŒ **Critiques & Points Faibles**

#### **Architecture**
- âŒ **Webhook blocking** - Bloque indexer si HTTP fail
- âŒ **No retry logic** - Ã‰chec webhook = perte donnÃ©es
- âŒ **Memory intensive** - Tous transfers chargÃ©s en RAM

#### **Data Quality**
- âŒ **amount vs amount_in_usdc confusion** - Deux champs similaires
- âŒ **Type mismatches** - amount TEXT dans v2 vs NUMERIC dans v1
- âŒ **No data validation** - Payloads bruts stockÃ©s

#### **Maintenance**
- âŒ **Configuration IPv4 hack** - Solution temporaire
- âŒ **No monitoring** - MÃ©triques limitÃ©es
- âŒ **TypeScript complexity** - Code difficile Ã  maintenir

### ðŸ”§ **AmÃ©liorations ProposÃ©es**

#### **PrioritÃ© Haute**
1. **Webhook Async & Retry**
   ```typescript
   // Queue non-blocking avec retry
   const webhookQueue = new WebhookQueue({
       retryAttempts: 3,
       backoffMs: 1000,
       deadLetterQueue: true
   })
   ```

2. **Data Validation**
   ```typescript
   // Schema validation avant stockage
   const transactionSchema = z.object({
       amount: z.number().positive(),
       price: z.number().min(0).max(1),
       amount_in_usdc: z.number().positive()
   })
   ```

3. **Memory Optimization**
   ```typescript
   // Streaming processing au lieu de load all
   const processor = new StreamingProcessor({
       batchSize: 100,
       memoryLimit: '512MB'
   })
   ```

#### **PrioritÃ© Moyenne**
4. **Dual RPC Endpoints**
   ```typescript
   // Haute disponibilitÃ© RPC
   const rpcEndpoints = [
       process.env.RPC_POLYGON_HTTP,
       'https://polygon-rpc.com',
       'https://polygon.llamarpc.com'
   ]
   ```

5. **Metrics & Monitoring**
   ```typescript
   // MÃ©triques dÃ©taillÃ©es
   const indexerMetrics = {
       blocksProcessed: new Counter(),
       transactionsParsed: new Counter(),
       webhooksSent: new Counter(),
       errors: new Counter()
   }
   ```

---

## ðŸ”„ 4. RELATIONS ENTRE SERVICES

### **Flux de DonnÃ©es Complet**

```mermaid
graph TD
    A[Poller] --> B[subsquid_markets_poll]
    C[Streamer] --> D[subsquid_markets_ws]
    E[Indexer] --> F[subsquid_user_transactions]
    E --> G[subsquid_fills_onchain]

    B --> H[MarketDataLayer]
    D --> H
    F --> I[Copy Trading Service]
    G --> J[Analytics Service]

    H --> K[Bot Commands]
    I --> L[Notifications]
    J --> M[Leaderboards]

    N[Redis Cache] --> H
    N --> I
    O[Supabase] --> K
    O --> L
    O --> M
```

### **DÃ©pendances & Synchronisation**

#### **Temps RÃ©el (Streamer)**
- **DÃ©pendance**: Aucune - autonome
- **Impact failure**: Prix non temps rÃ©el, fallback poller
- **Recovery**: Auto-reconnexion (backoff + jitter)

#### **Batch (Poller)**
- **DÃ©pendance**: Supabase healthy
- **Impact failure**: MÃ©tadonnÃ©es obsolÃ¨tes
- **Recovery**: Retry avec backoff

#### **On-Chain (Indexer)**
- **DÃ©pendance**: RPC Polygon + Supabase + Redis
- **Impact failure**: Copy trading arrÃªtÃ©
- **Recovery**: Restart indexer (state perdu)

---

## ðŸ“Š 5. ANALYSE COMPARATIVE

| Aspect | Poller | Streamer | Indexer |
|--------|--------|----------|---------|
| **Latence** | 60s | <10ms | <5s |
| **FiabilitÃ©** | Ã‰levÃ©e | Moyenne | Ã‰levÃ©e |
| **ComplexitÃ©** | Ã‰levÃ©e | Moyenne | Ã‰levÃ©e |
| **Maintenance** | Difficile | Moyenne | Difficile |
| **CriticitÃ©** | Moyenne | Ã‰levÃ©e | Ã‰levÃ©e |

### **Recommandations Globales**

#### **ðŸ”´ Critique (Setup)**
1. **Configuration centralisÃ©e** - Un seul endroit pour tous les services
2. **Health checks unifiÃ©s** - Dashboard commun pour monitoring
3. **Error handling standardisÃ©** - Patterns communs pour recovery

#### **ðŸŸ¡ AmÃ©lioration (Relations)**
1. **Service mesh** - Communication inter-services fiable
2. **Circuit breakers** - Protection contre cascading failures
3. **Metrics aggregation** - ObservabilitÃ© unifiÃ©e

#### **ðŸŸ¢ Optimisation (Performance)**
1. **Resource pooling** - Partage connexions DB/Redis
2. **Batch operations** - RÃ©duire nombre de queries
3. **Caching intelligent** - TTL adaptatif selon donnÃ©es

---

## ðŸŽ¯ CONCLUSION

### **Points Forts**
- âœ… **Couverture complÃ¨te** - Off-chain + on-chain
- âœ… **Temps rÃ©el possible** - Via WebSocket + webhooks
- âœ… **RÃ©silience** - Fallbacks entre services

### **Risques Majeurs**
- âŒ **ComplexitÃ© excessive** - Maintenance difficile
- âŒ **DÃ©pendances cachÃ©es** - Services interconnectÃ©s
- âŒ **Monitoring insuffisant** - Debugging difficile

### **PrioritÃ©s d'AmÃ©lioration**
1. **ðŸ”´ Simplifier architecture** - RÃ©duire complexitÃ©
2. **ðŸŸ¡ Unifier configuration** - Setup centralisÃ©
3. **ðŸŸ¢ AmÃ©liorer observabilitÃ©** - Monitoring complet

**Score Global: 6.5/10** - Architecture fonctionnelle mais nÃ©cessite simplification majeure.

---

*Document crÃ©Ã© le 6 novembre 2025 - Analyse dÃ©taillÃ©e des services de data ingestion*
